
\chapter{POND Evaluation}
\label{cha:cont4}
The previous in-lab study (the comparison between BAL-HEI-FBQI) clarified that some people preferred the detailed calorie lookup, while some were satisfied with the checkbox approach, which was in contrast to what I expected based on the BALANCE study, where many participants reported frustration with the BALANCE tool. This chapter presents the results of an evaluation of the POND tool described in the previous chapter. The reported results focus on what people entered, when they made entries and how it changed over time, and how they chose to create the entries.  

\section{Introduction}
The POND evaluation included both in-lab and \textit{in situ} components.  The in-lab study was similar in design to the BAL-HEI-FBQI study presented in Chapter \ref{cha:app_cont2_inlab_surveys}. Participants came to the lab, used the POND tool to make food entries specified on a card, then asked for feedback about the tool and experience. The in-lab portion was piloted, and the software was modified based on feedback from participants. The \textit{in situ} component had participants install the POND software on their  personal phone, and to use it to track what they eat for three weeks. 

I present the two studies in two sections: The first reflects the in-lab portion of the studies, and the second focuses on the real-world portion of the study. The in-lab portion of the study were the same (same data collected, same tasks used), with minor changes to the software between phases. 

One area of interest in this study was how people would personalize the POND software to make it personally interesting, important, and not too challenging. The in-lab portion of the study consisted of conditions that varied the number of components on the screen. This was to introduce participants to the idea of varying complexity and the customization. I believed that participants who could personalize the tool to something that fit within their goals, motivation and resources would be more satisfied with using the POND software to monitor their food intake. 

\section{Evaluation Organization}
This evaluation consisted of an in-lab pilot, then an in-lab and in situ  data collection. The software was slightly modified based on feedback from the participants in the pilot, then used for the rest of the study. The second part of the study consisted of participants performing the same tasks as in the pilot in-lab study, followed by them tracking their dietary intake using POND on their personal mobile phone. 

I report and discuss the data in terms of the in-lab portion and the in situ data. When discussing the in-lab data, I treat the pilot and final data in the same way. 

First, I describe the in-lab part of the study. Then, I describe the data collected, report the results and discuss. Next, I describe the in situ part of the study, the data collected in situ, report the results and discuss. Then, I discuss overall, addressing anything that is related between the in-lab and in situ portions. 

\section{In-Lab Evaluation}
\label{sec:inlabEval}
The purpose of the in-lab evaluation was to collect data to characterize how people use the POND software to enter known food tasks. 

There were two primary goals with this in-lab study. The first goal was  to collect data to characterize what kinds of errors people made when entering known food into the software. This addresses the usability of POND. To address this goal, I asked participants to enter given foods into the software. The second goal was to collect formative feedback on the concept of using POND to support the setting and monitoring of small goals. To address this goal, I changed the target goals throughout the study. 

\subsection{Experiment Design}
This in-lab evaluation is a single-factor, multilevel, within-subjects design. The single factor is the number of components enabled in the POND interface. There are four levels: Small (2 components), Medium (5 components), Large (9 components) and Full (all 13 components). The components for each condition were chosen randomly (without replacement) for each participant at the start of the study. All components were used at least once. Condition order was counterbalanced, and tasks were always presented in the same order. All of the target amounts for each component were calculated by the HEI-05 recommendations for an adult with a target calorie intake of 2000 calories. 

\subsubsection{Participants}
For the pilot study, 12 participants (6 male, 6 female) were recruited from email lists for a major university. All participants were students or instructors in a technology-centric field (Computer Science \& Engineering, Informatics,  and Human-Centered Design \& Engineering).  All reported using a cell phone several times a day, and all but one reported entering text on their cell phone several times per day (the other one reported entering text on their cell phone 1-2 times per day). All participants owned smartphones, 2 participants owned smartphones that did not have a touch-screen and 4 owned smartphones with QWERTY keyboards. 4 reported using a food diary previously. Of the 4, all had used a food diary on a smartphone, while one of those 4 had also used paper and either software or a website. When asked about health concerns and goals, 11 people indicated that they are interested in ``eating better'', 6 are interested in losing weight, 2 want to better control portion sizes, and 2 are interested in eating less or more. Other answers volunteered included ``Eating fewer processed foods'', and ``Eating only healthy/no junk food''. When asked to rate on a scale of 1-4 (very knowledgeable) how knowledgeable they are about food and nutrition (``including nutrients (fat, carbohydrates, fiber, vitamins, etc) and/or ingredients''), 5 reported themselves as a 2, 6 reported themselves as a 3, and 1 reported themselves as 4, or that they ``are expert/have spent much time understanding nutritional aspects of food.'' Participants in the pilot study were compensated for their time with a \$10 Amazon gift certificate.

22 people participated in the main study, 17 female and 5 male. Ages ranged from 21-64, and occupations varied. All participants reported using their cell phone several times a day, and all except 1 reported entering text on their cell phones several times a day (the remaining one entered text on their cell phones 1-2 times daily). All participants owned cell phones with touch screens, and 2 owned phones with an additional QWERTY keyboard. 19 participants reported that one of their health goals is to ``Eat better'', while 8 reported wanting to lose weight, 8 wanted to better control portion sizes, and 4 wanted to eat less or more. 6 people reported never tracking their food intake before, while 7 had used a smart phone, 8 had used a website, 8 had used paper, and 1 used some other software on their computer. 7 reported themselves as very knowledgeable about food and nutrition, 10 as fairly knowledgeable, 4 as not so knowledgeable, and 1 as fairly uneducated about food and nutrition.  

Participants were recruited from the community via craigslist posting, a recruitment request sent to a local mom email list, posters in the neighborhoods at cafes, bookstores, and athletic shops, as well as university email lists at the university that were different than the ones used in previous studies. The recruitment advertisement asked for people who were able and willing to journal their food for three weeks. Participants were compensated \$125 for their complete participation in the study. Participants self-reported that they had no medical concerns that impacted their food choices, and owned their own Android devices. 

Four participants completed the in-lab part, but did not complete the in situ part of the study. Of the four, two were unable to install the software on their personal devices, and two chose not to continue the study. 

\subsubsection{Procedure}
After completing the informed consent, participants were asked to complete a questionnaire to report demographics and background information, such as familiarity with mobile phones (including text entry), experience with using food diaries on different platforms (paper, web, mobile phone), and general health and wellness goals. 

Participants were briefed on the concept and benefits of a dietary pattern, and informed about the HEI-05 and all of the individual components. This information was similar to the details provided in the POND app (via the ``More details...'' button), but more detailed. A printed version was provided for reference. The app was explained to the participant, and some example entries were made. The participant was given time to use the app individually and ask any questions. 

The study consisted of 4 series of 5 tasks: 4 conditions with 5 tasks in each condition. Each condition represented roughly 1 day of food intake, with each task representing the content of 1 meal. Each task was presented on a card, with a single food on each line, and the amount printed below. Task cards had printed IDs, and the software was modified to ensure that input aligned with the correct task. For each task, participants were instructed to start the app and enter the food as desired (using either the +1 buttons or the database lookup), and hit the back button to exit the app when all foods on the card were entered.  At the end of each condition, participants completed a questionnaire that included TLX measures. 

Participants were asked to tell the administrator if they made a mistake, but not to fix it. The app was designed such that entering was very easy, but fixing a mistake took longer. For this study, I wanted to be able to capture the entry time not confounded by edit time. This approach allowed us to capture the distinction between an error the participant knew they made, and errors they were unaware of. 

At the end of all four conditions, participants completed a questionnaire that focused on overall feedback about the software. 
\begin{figure}	
\centering
	\begin{subfigure}[t]{1.25in}
		\centering
		\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[width=1.25in]{./images/cont4/small_condition}}
		\caption{Small condition.}\label{fig:small_cond}
	\end{subfigure}
\quad
\begin{subfigure}[t]{1.25in}
		\centering
		\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[width=1.25in]{./images/cont4/medium_condition}}
		\caption{Medium condition. }\label{fig:med_cond}
	\end{subfigure}
\quad
\begin{subfigure}[t]{1.25in}
		\centering
		\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[width=1.25in]{./images/cont4/large_condition}}
		\caption{Large condition. }\label{fig:large_cond}
	\end{subfigure}
	\caption{POND Home screens for each condition}\label{fig:3_1}
\end{figure}



\subsubsection{Tasks}
The tasks were the same as developed for the in-lab study reported in Chapter \ref{cha:app_cont2_inlab_surveys}. However, the tasks were developed using an earlier version of the food database (Nutritionist Pro Knowledge Base, Version 4.2) than was used for the POND app (Nutritionist Pro Knowledge Base, Version 4.4). One major modification from Version 4.2 to Version 4.3 was the ``Food Naming System''. New food names had a different ordering strategy than the previous one. An example is in Version 4.2, example food entries are ``Mashed Potatoes'' and ``Reduced Fat Milk, 2\%'', while corresponding entries in Version 4.3 (and later) are ``Potatoes, Mashed'' and ``Milk, Reduced Fat 2 \%''. The task cards had the Version 4.2 food names, so participants were looking for ``Mashed Potatoes'', but had to choose ``Potatoes, Mashed''. This could arguably more closely reflect a real world experience, where the individual needs to generate a query term.



\subsection{Measures and Data Collected}
As discussed in the previous chapter, the goal for this evaluation was to identify the patterns of use of the POND food diary \textit{in situ}. Experience with BALANCE and previous work suggested three weeks is a long time to journal food intake. POND was designed to require less time and effort to journal food intake. How long will users be able to continue journaling when using the POND software?

Other questions of interest included both usability and usefulness topics:
\begin{itemize*}
\item Will the ability to customize the interface to minimize it to the things a user cares
about (``goals'') impact the use of the tool?
\item If the default interface contains items in which users do not express interest, and are
just taking more time for entry, do they get rid of them and keep tracking?
\item Will users prefer to use the approach that takes less time, or captures more detail?
\item How will users use the food database lookup feature?
\item When will they use the food database rather than the +1 buttons?
\item What food items will they use the food database for?
\end{itemize*}

To address these questions, I collected the following data and measures: 

\begin{enumerate*}
\item \textbf{Entry Strategy.} How a task was entered: with just +1 buttons, just lookup, or a mix of the two. 
\item \textbf{Correctness/Errors. } A score from 0-2 representing the correctness the task entry. 
\item \textbf{Timing/duration.} Time spent in each condition. 
\item \textbf{TLX.} TLX \citep{Hart2006} measures reported after each condition. 
\item \textbf{Likes/dislikes} List of self-generated features liked and not liked about each condition. 
\item \textbf{Search terms.} A list of the searches executed for each task (food item). 
\item \textbf{Final questionnaire.} Includes questions about how likely the participant is to use the app for a given length of time, and how interesting and useful each individual component is. The entire questionnaire is documented in the Appendix. 
\end{enumerate*}


\subsubsection{Study Design Limitations}
As noted in Chapter \ref{cha:relatedWork}, the process of using written food names as tasks has the potential to include foods participants may not be familiar with. This is addressed by asking participants to report their familiarity with the foods at the end of each condition. The scripted nature of the study prevented participants from exploring features of the software and becoming more familiar with it. As with most food diaries, POND is designed to be quicker and easier over an extended period of use, thus in-lab studies may reflect the novice effect. Finally, all food diaries evaluated in a lab setting do not reflect the reality that  situation and context matter when using a food diary in situ. 

\subsection{In-Lab Results}

\subsubsection{Entry Strategy}
Entry strategy reflects how participants made entries. In general, I were interested in when people made the decision to use the +1 buttons or the lookup feature to enter a food. I found participants tended to have their preference for using +1 or lookup. The results are summarized in \ref{tab:inlab_strategy}. 

In the table, ``Only +1'' and ``Only Lookup'' mean that the participant only used the +1 or lookup entry process, respectively. The ``Mostly'' designation reflects that a majority of the entries were made with the specified process. ``Mix'' did not reflect a strong preference for either strategy. 


% Table generated by Excel2LaTeX from sheet 'PONDstrategy_inlab'
\begin{table}[htbp]
\small
  \centering
  \caption{Entry Strategy}
    \begin{tabular}{rrrr}
    \toprule
          & Num Pilot Ppts & Num Ppts & Total \\
    \midrule
    Only +1 & 3 (25\%) & 7 (29\%) & 10 (28\%) \\
    Mostly +1 & 1 (8\%) & 3 (13\%) & 4 (11\%) \\
    Mix   & 1 (8\%) & 10 (42\%) & 11 (31\%) \\
    Mostly Lookup & 4 (33\%) & 4 (17\%) & 8 (22\%) \\
    Only Lookup & 3 (25\%) & 0 (0\%) & 3 (8\%) \\
\midrule
    Total & 12 (100\%) & 24 (100\%) & 36 (100\%) \\
    \bottomrule
    \end{tabular}%
  \label{tab:inlab_strategy}%
\end{table}%


 \textbf{Pilot data:}
Three participants chose to use the +1 buttons exclusively, while one other participant used the +1 buttons primarily, but tried the lookup function a bit. Of the remaining 8 participants, 4 mostly used the lookup function, 1 used lookup or a combination exclusively, and the last 3 used a combination of the three strategies. 

\textbf{Full data:}
Seven participants chose to use the +1 buttons exclusively, while three other participants used the +1 buttons primarily, but tried the lookup function a bit. Of the remaining 14 participants, 4 mostly used the lookup function,  none used lookup or a combination exclusively, and the last 10 used a combination of the three strategies. 


\subsubsection{Search Terms}
% Table generated by Excel2LaTeX from sheet 'search terms'
\begin{table}[bthp]
\small
  \centering
  \caption{Common queries}
    \begin{tabular}{rr}
    \toprule
    Query term & Number of people who used it \\
    \midrule
    doritos & 11 \\
    baking chocolate & 10 \\
    egg   & 10 \\
    wheat thins & 9 \\
    fiber one & 9 \\
    pepperoni & 9 \\
    starbucks & 9 \\
    mashed potatoes & 8 \\
    wheat crackers & 8 \\
    don miguel & 8 \\
    le gout & 8 \\
    salad & 8 \\
    milk  & 8 \\
    \bottomrule
    \end{tabular}%
  \label{tab:inlab_commonqueries}%
\end{table}%


% Table generated by Excel2LaTeX from sheet 'search terms'
\begin{table}[htbp]
\small
  \centering
    \begin{tabular}{rrrr}
    \toprule
    description & Number of people who used it &       & length  \\
    \midrule
    pauly county line advantage swiss cheese & 1     &       & 40 \\
    pepperidge farm crusty Italian garlic & 1     &       & 37 \\
    bag n season pork chop seasoning mix & 1     &       & 36 \\
    keebler zesta soup \& oyster crackers & 1     &       & 36 \\
    low calorie thousand island dressing & 1     &       & 36 \\
    pepperidge farm crusty utilian bread & 1     &       & 36 \\
    \bottomrule
    \end{tabular}%
  \caption{Long queries}
  \label{tab:inlabLongQueries}%
\end{table}%


When participants chose to make food entries by using the food database, they generated a query. For the non-pilot study, I saved all of the search terms and report on them here. Terms that appeared to be part of the practice tasks were excluded. 

Overall, the 24 non-pilot participants made 650 queries from 273 unique phrases. 130 query terms were used by more than one person. All phrases with 8 or more queries are listed in Table \ref{tab:inlab_commonqueries}. Of these 13 queries, 3 represent foods that most likely fit into a single category (egg, salad and milk). It is possible that the salad query was used to find a ``Caesar salad'' entry (salad greens plus dressing and croutons), rather than simply salad greens, which could be counted with just one food group. The other 10 most common queries represent foods that are primarily packaged and prepared. The 6 longest queries are listed in Table \ref{tab:inlabLongQueries}. The mean length of query is 13.6 characters. 

\subsubsection{Likes and Dislikes}
Participants were asked to provide three things they liked and disliked about the interface after each condition. Here, the responses are summarized and some common themes identified. Since the presentation of the different conditions was counterbalanced, the comments reflect that some participants saw the conditions growing or reducing from condition to condition. 

\textbf{SMALL CONDITION (2 components).}
Some people liked that it was so short (``it was trying to just keep track of my best and worst food choices''), while others felt it was too limiting ``I wanted to put all of the food I ate into categories, not just some of it''. 

\textbf{MEDIUM CONDITION (5 components).}
Fewer items, so it takes less time, but more ``mentally taxing'', because they ``had to think more about whether a food contained parts of the specified categories''. One person mentioned s/he liked ``Quicker entry for foods I've encountered before''. 

\textbf{LARGE CONDITION (9 components).}
Most comments are similar to the other conditions. Comments unique to this condition include: ``I don't find any different approach with the first approach, I just realize that there is no meat nutrients therefore everytime [sic] i see meat then i will just skip it.'' ``Didn't feel much difference between the first and second approach. However, I did like that there were different categories.''

\textbf{FULL CONDITION (all components).}
Participants liked having all the categories. They reported that it made them more informed and ensured they were accounting for all of the parameters. 

\textbf{ALL CONDITIONS.}
Feedback about the POND software in general applied to the database, food entry process, and colorful visualization. Participants liked the ability to search and that the database had so many items. However, some felt the results were too wordy and that it took too long to find the exact food and portion size. For some people, even though it takes ``too many steps to enter a food item'', they would prefer to be exact to get the analysis. It was ``fun to hit the +1 buttons''. The colorful visualization helped people to figure out what food falls into what category. It also provided a ``meter'' that presented the nutritional value of what was eaten. One common complaint was the lack of feedback or certainty about whether an entry was correctly entered. 


\subsection{Discussion--- In lab}

The goal of the in-lab portion of the POND evaluation was to characterize how people used the diary to create known food entries. I looked at the strategy participants used to make an entry, the search terms used for known foods, and the things participants liked and disliked. 

I saw that overall, participants were split on how much to use the +1 buttons rather than the lookup feature for creating food entries. The distribution of strategies people used in the pilot study is different than the strategies people used in the full study. More participants in the pilot utilized the search feature, while participants in the full study relied more on the +1 entry strategy, although they also appeared to be more likely to combine the strategies. However, pilot participants were quite vocal about their negative impression of the search feature, which is why it was modified before the full study. Those pilot participants, who had the poor search implementation, used it much more than the participants who had the revised search function. 

The difference in strategy proportions for pilot and non-pilot could reflect the different populations. The pilot study consisted primarily of graduate students in fairly technical fields, while the study population consisted of people from the general population. 

The randomization of the components to the conditions could impact the choices that participants made in regards to using the +1 versus lookup. The Full condition (which contained all components) are comparable across all participants, but in the Small and Medium conditions, it is possible that the components contain either just easy food groups (Fruit, Veggies) or all nutrients (Sodium, Sugar), which are known to be more challenging to count, and people report using the lookup feature for them.

Reviewing the most common search results indicates that participants are searching for unfamiliar, processed foods that are challenging to identify components for. These foods also tend to be higher in sodium and solid fats, which are difficult to estimate without looking up. The most common searches are one or two words. One thing I are unable to evaluate is if  short searches result in a hit more frequently than long searches. The long searches are very similar to the text that appeared on the task card. This reflects how the task cards prime the users to choose search terms. 

The reported likes and dislikes revealed that for this general population that is not necessarily in the process of changing their eating behaviors, the changing of the components on the home screen is not meaningful. In particular, participants felt it odd that some foods might not be counted at all in a given condition. For example, in the small condition, an apple might not be counted if the two components are `sugar' and `sodium'. This is consistent with feedback in the food index comparison study. However, some people noted that it might be worthwhile to spend a short time focusing on one particular component. Overall, people liked the analysis of the food intake pattern. 

\section{In-situ Study}

The second part of the POND evaluation consists of an \textit{in situ} study. 

\subsection{Evaluation Design}
This study is a 1x2 between-subjects design. The two levels were whether participants could change the components on the front screen or not. This included any modification to the goals, either the listed components or the amounts for each component. 

\subsubsection{Procedure}
Participants performed the in-lab study as described in Section \ref{sec:inlabEval} above. When they were completed with the in-lab part of the study, the software was installed on the participant's personal Android phone. Participants were shown software features not addressed in the lab part of the study, such as how to edit entries, create custom meals, how to view a list of daily entries over time, and the weekly overview. They were given a chance to interact with the software, and ask any questions. They were told to explore the software for the rest of the day, but that the study started the next morning. They were told to customize the components and goals as they desired, and that the serving sizes were there as a guideline, but that they could choose to count the servings as they felt comfortable. 

At the end of every seven days, participants were asked to send a copy of their data to the research team, and asked to fill out a survey about their experience for the previous week. 

At the end of the three week period, participants returned to the lab for a guided interview about their experiences. At that point, the software was removed from their personal device. 

\subsubsection{Study Design Limitations}

This study does not try to identify the accuracy of the food entries. Other studies attempt to do this with the use of 24-hr recalls (single or periodic). 

\subsection{Measures and Data Collected}
As discussed earlier, the goal of this study was to understand how the POND software was used over time. 

\begin{enumerate*}
\item Data logged on phone: 
\begin{itemize*}
\item \textbf{Entry strategy.} The number of entries made with the +1 buttons and with the lookup feature. 
\item \textbf{Food search.} The number of food searches performed. Which terms were used. 
\item \textbf{Component summaries. } How many of each component were entered overall. 
\end{itemize*}
\item Full study interim surveys:  (1 wk, 2 wk, 3 wk)
\begin{itemize*}
\item \textbf{Likes and dislikes.} Three things you do and do not like.
\item \textbf{Effectiveness. } Where is it effective or not effective.
\item \textbf{Difficulty and Interesting. }  Which components were difficult or interesting to monitor.
\item \textbf{Self-efficacy.} Self-efficacy questions at the end of each week. 
\end{itemize*}
\item Correlation between weekly self-report of use and actual use. 
\end{enumerate*}

\subsection{Results}


\subsubsection{Entry strategy}
One challenge of \textit{in situ} food diary studies is the inability to know what is actually eaten and how it compares to what is entered. In the lab portion of the study, I could calculate whether a given meal was entered using only +1 buttons, only food lookup, or a mixture of the two. This calculation is impossible for this part of the study. Instead, I report on how many +1 versus lookup entries participants made over the entire three weeks. 

Number of entries made is counted in two ways. The ``Number of Entries'' is the straight number of entries made, either the number of times a +1 button is pressed or the number of entries of food from the database. ``Collapsed Entries'' collapses the +1 entries into entries made within 15 seconds of each other. The former counting strategy may prove an unfair comparison of the +1 and lookup entries, while there is no proof that the latter improves the value of the comparison. Table  \ref{tab:strategy_table} shows only the sums for the week. 

Two participants  preferred using the lookup feature. After 8-10 days of use, the app stopped behaving properly for them. They eventually reverted to using the +1 buttons rather than the preferred lookup, and reported that they would have preferred to keep looking up. Their databases had become corrupted. Their data in included in Table \ref{tab:strategy_table}, where the rows for p2001 and p5002 show zeroes for week one and two. In Table \ref{tab:strategy_table}, the ``Alt. Mean'' row shows the means calculated without those participants for week one and week two. 

Overall, only nine participants created a substantial number of entries from the food database during the study.  

% Table generated by Excel2LaTeX from sheet 'Sheet8'
\begin{table}[htbp]
\small
  \centering
  \caption[Entry Counts. ]{Entry Counts. This table shows the weekly entry counts for each participant. }
    \begin{tabular}{lrrrrrrrrrrrr}
    \toprule
          & \multicolumn{4}{c}{Collapsed} & \multicolumn{4}{c}{Total}     & \multicolumn{3}{c}{Food Entries} &  \\
    \midrule
          & \multicolumn{3}{c}{Week ID} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{Week ID} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{Week ID} &  \\
    Ppt ID & 0     & 1     & 2     & Total & 0     & 1     & 2     & Total & 0     & 1     & 2     & Total \\
    p1001 & 27    & 22    & 11    & 60    & 148   & 156   & 78    & 382   & 3     & 3     & 0     & 6 \\
    p1003 & 58    & 47    & 54    & 159   & 168   & 164   & 161   & 493   & 3     & 0     & 1     & 4 \\
    P1004 & 30    & 14    & 9     & 53    & 41    & 24    & 15    & 80    & 0     & 2     & 0     & 2 \\
    p2001* & 36    & 0     & 0     & 36    & 70    & 0     & 0     & 70    & 29    & 0     & 0     & 29 \\
    P2002 & 37    & 32    & 28    & 97    & 115   & 160   & 143   & 418   & 8     & 3     & 8     & 19 \\
    p3002 & 50    & 48    & 34    & 132   & 112   & 146   & 68    & 326   & 8     & 4     & 18    & 30 \\
    P3011 & 39    & 23    & 32    & 94    & 65    & 48    & 66    & 179   & 18    & 0     & 7     & 25 \\
    p3012 & 44    & 32    & 37    & 113   & 70    & 61    & 77    & 208   & 0     & 0     & 0     & 0 \\
    P3013 & 20    & 17    & 13    & 50    & 117   & 140   & 120   & 377   & 0     & 0     & 0     & 0 \\
    p3014 & 30    & 15    & 16    & 61    & 140   & 101   & 111   & 352   & 0     & 0     & 0     & 0 \\
    p3021 & 76    & 60    & 45    & 181   & 281   & 327   & 228   & 836   & 7     & 0     & 0     & 7 \\
    p3022 & 45    & 43    & 24    & 112   & 133   & 113   & 118   & 364   & 18    & 16    & 3     & 37 \\
    P3023 & 35    & 34    & 33    & 102   & 109   & 114   & 53    & 276   & 1     & 8     & 20    & 29 \\
    P3024 & 26    & 29    & 31    & 86    & 34    & 48    & 41    & 123   & 14    & 12    & 21    & 47 \\
    p4001 & 63    & 40    & 46    & 149   & 166   & 141   & 179   & 486   & 1     & 2     & 5     & 8 \\
    p4002 & 52    & 39    & 33    & 124   & 165   & 125   & 116   & 406   & 0     & 0     & 0     & 0 \\
    P4003 & 43    & 30    & 33    & 106   & 118   & 105   & 112   & 335   & 0     & 1     & 2     & 3 \\
    p5001 & 41    & 33    & 36    & 110   & 139   & 118   & 84    & 341   & 1     & 1     & 17    & 19 \\
    p5002* & 48    & 11    & 0     & 59    & 88    & 27    & 0     & 115   & 10    & 1     & 0     & 11 \\
    p5003 & 28    & 23    & 26    & 77    & 123   & 127   & 153   & 403   & 1     & 2     & 1     & 4 \\
\midrule
    Mean  & 41.4  & 29.6  & 27.05 & 98.05 & 120.1 & 112.25 & 96.15 & 328.5 & 6.1   & 2.75  & 5.15  & 14 \\
    Alt. Mean & 41.4  & 32.3  & 30.1  &       & 120.1 & 123.2 & 106.8 &       & 8.7   & 4.6   & 9.4   & 17.5 \\

    \bottomrule
    \end{tabular}%
  \label{tab:strategy_table}%
\end{table}%


\subsubsection{Search Terms}
The searches that participants make indicate what kinds of foods they were concerned about. I report how many participants made queries, how many queries were made, how many queries or searchers were repeated, and the longest queries. 

% Table generated by Excel2LaTeX from sheet 'Sheet10'
\begin{table}[tbhp]
\small
  \centering
  \caption{Common Searches}
    \begin{tabular}{rrrr}
    \toprule
    description & Number of searches for this & Total number of Results & Length of query \\
    \midrule
    beer  & 12    & 115   & 4 \\
    egg   & 9     & 398   & 3 \\
    avocado & 8     & 17    & 7 \\
    chipotle & 6     & 85    & 8 \\
    coffee & 6     & 325   & 6 \\
    oatmeal & 6     & 297   & 7 \\
    butter & 5     & 735   & 6 \\
    margarine  & 5     & 258   & 10 \\
    mocha & 5     & 123   & 5 \\
    Peanut butter & 5     & 304   & 13 \\
    pizza & 5     & 1102  & 5 \\
    twix  & 5     & 9     & 4 \\
    \bottomrule
    \end{tabular}%
  \label{tab:insituCommonQueries}%
\end{table}%


% Table generated by Excel2LaTeX from sheet 'PONDsearches_insitu'
\begin{table}[tbhp]
\small
  \centering
  \caption{Long searches}
    \begin{tabular}{rrr}
    \toprule
    description & length of search & total results \\
    \midrule
    healthy choice lemon garlic chicken & 35    & 0 \\
    whole wheat chocolate chip cookie & 33    & 0 \\
    chocolate chip granola bar & 26    & 15 \\
    whole wheat English muffin & 26    & 4 \\
    chocolate covered raisins & 25    & 3 \\
    trader Joes chicken gyoza & 25    & 0 \\
    \bottomrule
    \end{tabular}%
  \label{tab:insituLongQueries}%
\end{table}%


There were 465 food queries from 308 unique terms, made by 18 participants. 76 query terms were used more than one time, with 42 query terms used by more than one person. 78 searches returned no results. The mean number of results is 86. The most common query terms are summarized in Table \ref{tab:insituCommonQueries}.

Overall, the mean length of query term was 10.03 characters, with the longest term being 35 characters. The longest query terms are noted in Table \ref{tab:insituLongQueries}.

Further details about the queries are included in the appendix. 


\subsubsection{Overall Component Counts}
The overall component counts is the sum of each component over all three weeks. This is the final component counts that includes entries made either by the +1 buttons or via the food lookup feature. Table \ref{tab:insitu_component_counts} shows the final component counts for each participant. Each polar plot represents a single participant. The length of the pie slice represents how much of the component was entered. Some of the participants (e.g. P1004 and P2001) show fewer entries overall. Other participants (e.g. P1003 and P3021) made more entries. P3021 shows a large Protein count. This reflects a misunderstanding the participant reported in the final interview. He showed confusion about a block representing a gram of protein rather than an ounce of meat, beans or single egg. P3013 shows that she chose not to monitor Oils, Fats, Sodium or Sugar. 

Figure \ref{fig:ppt_overview} shows a chart displaying the daily component count for each category for the entire 3 weeks, for a single participant. This provides a sense of the pattern of use for this participant, and what s/he was focused on each day. 

\begin{figure}[ h ]
\begin{center}
\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[ height=5in ]{./images/cont4/ppt_overview}}
\label{fig:ppt_overview}

\caption{Overview of Ppt 1001}
\end{center}
\end{figure}

%\input{cont4_insitu_component_counts.tex}

\begin{table}[htbp]
\small
  \centering
  \caption{Final Component Counts. }
    \begin{tabular}{p{1.25in}p{1.25in}p{1.25in}p{1.25in}}
    \toprule
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p1001}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p1003}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p1004}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p2001}\\
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p2002}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p3002}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p3011}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p3012}\\
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p3013}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p3014}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p3021}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p3022}\\
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p3023}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p3024}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p4001}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p4002}\\
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p4003}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p5001}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p5002}&
\includegraphics[width=1.25in]{./images/cont4/polar_charts/p5003}\\
\bottomrule
\label{tab:insitu_component_counts}
\end{tabular}
\end{table}

\subsubsection{Guided Interview}
At the end of the study, participants returned to the lab to conduct an in-person semi-structured interview about their use of the tool. The interview protocol is attached as an appendix, as is the coding documentation. Below, I report some of the responses to selected questions. 

\textbf{Were you able to make food entries right after you ate? When you were not able, why not?}
This question resulted in a wide range of responses. Some people reported using it consistently after eating, while others made comments that some meals were easy to remember to enter because they are at times that their routine is consistent or predictable; other meals were at times when the routine is unpredictable. Participants also commented about how their schedules changed week to week: participants who had been traveling noted that their schedules were just off overall. Some people changed schedules over the study: one participant noted that there was one week of classes that was ``predictably crazy'', one week of vacation that was relaxed and easy to pay attention, and one week of starting an internship rotation in a venue that was high-pressure and busy, where it was hard to remember or want to think about entering. Other issues that prevented people from making entries immediately after eating included that their phone wasn't with them (frequently commented that it was elsewhere, charging) or they were doing something else while eating and forgot. 

\textbf{What were 3 things you really liked about this diary?}
Many comments were about what features people would like. An improved food database and searching process were frequently mentioned. People also wanted to track their water intake, and some people wanted some notion of calorie intake. In regards to calories, some people wanted detailed caloric information, while others reported being comfortable with an estimate. People also talked about wanting more customization: the order of the items on the screen, the background color, which items appear on the front screen. A couple of people volunteered the notion of a ``Junk food'' button-to indicate that they had something with low nutritional value, but with less detail than specifying how much sugar, salt, or fat it contains. 

Our participants liked the distinction between whole grains and refined grains, and dark green/orange vegetables and all other vegetables. They also liked the overview or analysis this approach provided. The color coding of the different components appeared to support the analysis. Many people had trouble tracking sugar, sodium and fats, which is consistent with previous work of tracking nutrients. Due to this, some people chose to ignore the nutrients. In addition to the inherent challenge of tracking nutrients, some participants noted philosophical objections to monitoring the nutrients. One participant expressed concern about current nutrition recommendations to drastically restrict fat and salt. Other participants expressed a perspective that being too concerned about the accuracy of the nutrients in particular could reflect an unhealthy obsession. 

When asked about things people didn't want to record, one response was ``things I wasn't sure how to spell''. 

\textbf{Was there a pattern, rule or guideline you used to decide when to search for a food rather than enter it via ``+1''s? }
Participants reported three primary approaches to creating entries. Some participants preferred looking foods up in the database, but ended up using the +1 feature due to difficulties  in the lookup process. Difficulties included: uncertainty of what phrase to use to find the food; inability to find a food they wanted; not trusting the results of the database (e.g., Subway sandwich with no grains); and occasional software malfunction. One participant reported consistent software crashes when looking up food in the database. This participant resorted to using the +1 buttons to continue study participation, but reported preferring the lookup. Other participants chose to use the +1 feature as much as possible. Finally, some participants took the approach of looking up anything that was packaged, ``had a barcode'', or prepared at a restaurant, while using the +1 for anything self-prepared. 

\textbf{Are you currently satisfied with your nutrition behaviors?}
The first few interviews revealed that our participants had different self-assessments of their eating behaviors. This question was added in to attempt to capture this information from all participants. Some participants demonstrated a high degree of nutritional literacy and felt confident that their eating behaviors reflected this knowledge most of the time. Others demonstrated nutritional literacy, but indicated that they knew their current eating behaviors were not consistent with known recommendations. Finally, some participants did not reflect awareness or concern about their eating behaviors in general. 

\textbf{Did keeping a food record change how you ate? How so?}
Some people talked about how the tool helped them to think more about the choices they were making, and they ended up eating more whole grains, choosing more vegetables, eating or buying more fruits, or choosing not to eat something that would increase the yellows and orange categories. Some people were surprised about some of the nutrients they encountered: things like sugar in slushy, sodium in soup, or that they really did not consume as many whole grains as they believed they did. 


\textbf{Would you recommend POND to friends? Are there other people you would recommend this tool to?}
When asked who they would recommend this tool to, many people volunteered that it would be really good for senior citizens, because it is so easy to use and keeps things so simple. Others indicated that they thought it would be good for busy mothers and caregivers, not just to track for themselves but to also keep track of what children (or family members) eat. They felt POND could support communication about nutrition behaviors among all family members. People also thought it might be good for people who were just starting to try to figure out how to make changes in their diet. Two participants felt that POND would  not be useful for anyone because it did not provide enough detailed information. These two participants later reported concern about specific carbohydrate counts and calorie counts, respectively. Other participants thought POND could be useful to anyone, because it is simple, provides high level feedback,  and supports awareness of one's dietary patterns. 

\textbf{Did you use the location tagging feature? }
I asked people to volunteer coarse location information for entries. No one did so. In general, the feedback on this feature was that participants did not believe location mattered. Their perspective was that they did not care about analyzing where they ate food, and it was just another thing to do, so they chose not to do it. A couple of people volunteered that they might be able to imagine there are patterns in the data, but they did not think there was likely any insight or value from it. 

\textbf{Why did you choose to participate in this study?}
Finally, I asked participants why they participated in the study. Some people did it purely for personal interest, stating that generally they are interested in tools to support their own health and wellness, while others did it just for the money. A couple of people had a balance of the two. 

One area of confusion was not directly identified by a question, but emerged in the interview. 
Many participants were unclear if the goals were to be attained or moderated. The POND interface design specifically did not reflect this. The design rationale was that when POND is used in the greater context of a goal-oriented dietary change program, the goals would be personal and self-identified. Therefore, the user would know whether an individual component goal was to be attained or moderated. Since this trial did not directly evaluate the goal-tending support, the lack of an indicator was problematic. The Fruit Juice component was particularly confusing: some participants noted that they do not usually consume fruit juice, and they were not sure if the dietary guideline was making the recommendation to drink more juice. In fact, the guideline was to restrict juice intake, but when juice is chosen, ensure that it is 100\% juice. 


\subsection{Discussion}
The goal of the \textit{in situ} POND evaluation was to characterize how people used the POND software to monitor the foods they eat in the context of their daily lives. I looked at how often people used the +1 buttons to make an entry rather than looking up a food in the database, the searches that were made and reported on search characteristics, the total number of components that were entered overall, how many entries were made each week, and what participants told us about their routines throughout the study period. 

Nine participants used the food lookup on a regular basis. However, no one refused to make entries with the +1 buttons, even if they reported preferring the feeling of accuracy associated with using the lookup feature. This could indicate that people found the +1 approach beneficial at least part of the time. 

As in the in-lab study, I see that the commonly performed searches include foods that are difficult for people to know how to count in terms of the HEI components. 

The frequency of the search for `egg' reflects a problem with the ``Protein'' category. The category is formally called ``Meat, Beans, and Eggs'', but I made a design decision to shorten the title to ``Protein'' to fit the mobile device screen better. This resulted in participants expressing confusion about the Protein category. Participants reported counting protein bars and protein powder under the ``Protein'' category. The appropriateness of counting protein bars and powders in the Protein category depends on the source of the protein: Whey protein is a dairy derivative, while soy protein comes from beans. Another way the Protein category caused confusion was in terms of serving size. Protein is a macronutrient, and is reported on nutrition labels in grams. The HEI ``Meat, Beans and Eggs'' category (called Protein in POND) is to be counted in terms of ounces of meat, number of eggs, or servings of beans. Since one ounce of meat has about 7 grams of protein, mistakenly counting `grams of protein' rather than `ounces of meat' can lead to bad numbers. 

The overall component counts for each participant show that people probably did not count the nutrients correctly. The nutrients (sodium, sugar, solid fats, oils) are also moderation components, which means that in general, most people eat too much of those items, and the HEI recommends working to moderate one's intake of these items. However, all of the counts are quite low in relation to the targets. Although the target amounts may be high for this population, it is more likely that participants focused on the easy to use +1 buttons for counting the food group components. This is consistent with the entry strategy results, which showed more use of the +1 buttons than the lookup approach. This is also consistent with what participants reported in the final interview-- that the nutrient components were challenging to count. 

The final interviews revealed that many participants were confused about the target amounts. Although it was addressed in the initial launching of the study, participants were not always sure if all of the targets were to be attained or moderated. Additionally, some participants seemed to think that some of the components were not defined properly. One participant in particular was concerned about the protein target, in regards to the concerns discussed earlier. The concern about the targets is reasonable, as they were defined based on a ``typical'' American who requires 2000 calories each day. The targets were not modified for individuals, and as many of our participants were adult women, it is likely the targets were too high for them. 

The interview asked people to report on how typical their schedules and routines were over the study period. Some people reported fairly consistent routines that were typical, while others reported that some of the weeks were inconsistent and atypical (going on vacation, starting a new job). Most participants were able to characterize when they were able to consistently make food entries in a timely manner (that is, shortly after eating), and it usually had to do with their routine. However, each person had different characteristics about their routine that made it easy or challenging to make timely entries: some people were very busy at work, but structured at home; for others it was the opposite. Given the opportunity, no one chose to track the location of where they were making entries. This is interesting, because participants reported not wanting to track location because they did not think it would be an informative or useful piece of information. However, from a research or design perspective, what they were telling us was that their ability to make entries depending on their location. 

\section{Overall POND Observations and Discussion}
The POND evaluation included two phases: an in-lab phase where I could focus on the use of the POND app to enter known food items, and an \textit{in situ} phase where I could better understand how POND performed as a self-monitoring tool within the context of a user's real life. In both phases, I looked at how people created entries (using the +1 buttons or looking foods up in the database); what searches were made; and what components people focused on and used to make entries. 


\subsection{How Did Users Make Entries? }
I first looked at the strategies people used to create food entries. The strategy is either the streamlined +1 strategy, or the more detailed, accurate lookup strategy. 

The in-lab data indicated that individuals fall into one of three groups: those that value the quick and easy overview entry, those that value the accuracy of the lookup, and those that are willing to combine strategies. In the context of the real world, I see that more people default to the use of the +1 entries. Generally, I saw that participants (consistent with what they reported) used the lookup feature to enter processed and prepared foods. Also, participants in the in-lab study made 650 queries to look up 4 days of food, while for the entire 3-week study, only 465 queries were made. The increased reliance on the +1 buttons and the small number of queries made could reflect a growing familiarity with the counting scheme, as well as being more familiar with the food one eats in general (as opposed to the food in the tasks that could be less familiar). 

\subsection{What Entries Did Users Make?}
I next looked at what entries people made. In the in-lab portion of the study, the foods were defined. This means that the users had no control over what to enter into the software. Instead, I look at what queries they made when they chose to use the food lookup feature. I could compare the queries made in the in-lab study to those from the \textit{in situ} study. 

When I look at the queries that people made in the in-lab versus \textit{in situ} conditions, the \textit{in situ} searches were shorter (mean = 10.03 versus 13.6 characters, longest = 35 versus 40 characters). This could reflect that participants were more familiar with the foods they were eating, and not impacted by the long given names in the in-lab tasks. 

It is challenging to identify what foods were entered in the \textit{in situ} study. Some participants made entries via lookup, but most participants did not make most entries via lookup. They used the +1 approach instead. For this discussion, I look at the final component counts to reflect what participants entered into the software. 

The final component counts show that participants probably did not make the nutrient (sugar, sodium, solid fats, oils) entries consistently. Some people specifically chose to remove these components from the list of goals and not monitor them, while others may have decided that they were not worth the effort to monitor. User feedback indicates that even when participants thought these nutrients were worth monitoring, they chose not to because it was too challenging. However, consistent with the previous in-lab study, people seem to want a reminder of the components that they should moderate (restrict), even if they decide they do not want to actively monitor it. 

\subsection{When Did Users Make Entries?}
Finally, I looked at when participants made entries. This included both timestamps of entry creation and self-report of when participants made entries. 

The feedback from participants about when they made entries, and when it was challenging to make entries, was valuable. The focus group feedback from BALANCE revealed that people wanted to use small stretches of time when they were not doing much else to create food diary entries. There were a number of reasons for this. However, feedback from the POND study indicates that users were willing to make time for creating entries, as long as they could fit it into their schedule. That is, when participants had consistent, predictable schedules, users made time to create food entries. When the schedule was less predictable or consistent, users found it difficult to remember to create food entries at the time of eating. 

\subsection{Overall}
Participants in the \textit{in situ} study reported that using the POND software made them more aware of the processed foods that they ate. This appeared to include both which eaten foods were processed and how many processed foods they ate overall. Participants reported using the food lookup feature more frequently for processed or prepared foods, and the +1 feature for entering foods they prepared themselves. User awareness of their consumption of processed foods is important because people in general tend to be unaware of how many processed foods they eat. Processed foods also tend to contain more salt, sugar and fat than self-prepared foods. Participants in the POND study reflected surprise at the nutritional content of prepared foods. In particular, the component-based visualization appeared have more impact than the same numbers reported on the nutrition label of a food. A processed food entry such as a Slushie would show on the list of components as many sugar blocks, but no other blocks. This helped to put into perspective how processed foods impacted the moderation goals, but had no impact on the attainment goals. 

The component-based approach of analyzing processed foods helped participants to better understand what a given food was composed of. One example to note is that of Doritos, a food in one of the in-lab tasks. Sometimes participants noted that they were not sure how to count Doritos. Doritos happen to consist of a grain (corn), fat and salt. This awareness could help build nutritional literacy over time. This is consistent with participants who suggested that POND would be a good tool for people just learning about nutrition or just beginning a dietary change program. 

The awareness of processed versus self-prepared foods is one reason participants liked the overview analysis the POND tool presented. Participants noted that the visual nature of the gray and colored blocks helped them to stay aware of their overall consumption over time. Users became more aware of when their fruit and vegetable intake fluctuated. Touching the +1 button to declare that they ate ``something healthy'' was considered satisfying. In particular, one participant noted that she preferred the lookup strategy to ensure the accuracy of her diary, but she appreciated that the overview visualization allowed her to ensure that her dietary intake was well-balanced. 

Burke et al \citep{burke_experiences_2009} found that participants in a study focused on the impact of self-monitoring as part of a weight loss program fell into three groups: highly involved, nominally involved, and negligibly involved. In contrast, the participants in the POND study were all either nominally involved  or highly involved in the self-monitoring process. This difference could be related to the distinction of self-monitoring as part of a weight loss program, or due to the alternative input strategy that POND provided. 

Early evaluation shows that POND appears to support various user characteristics. Follow-on work will incorporate a more rigorous quantification of those characteristics. Next steps for the POND project include evaluating its use by participants with specific, stated goals and in specific stages of change as defined by the transtheoretical model \citep{prochaska_transtheoretical_1997}. 

\section{Summary}
In this chapter I described the evaluation of the POND food diary. The evaluation consisted of both in-lab and \textit{in situ} parts. An initial analysis focused on how participants made entries, what they made entries of, and when they made entries. I found that participants had preferences for either detailed, accurate records and were willing to take the time to look foods up in the database, or were satisfied with the overview analysis and only used the +1 buttons. Participants appreciated that the +1 buttons made the process to enter some foods easier and faster. Participants also provided rich descriptions of the context of when they were able to make entries, or when there were barriers to making entries. 
