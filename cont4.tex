
\chapter{Contribution 4: POND Evaluation}

The previous in-lab study (the comparison between BAL-HEI-FBQI) clarified that some people preferred the detailed calorie lookup, while some were satisfied with the checkbox approach, which was in contrast to what we expected based on the BALANCE study, where many participants reported frustration with the BALANCE tool. This chapter focuses on the evaluation of the POND tool described in the previous chapter, which aims to understand if these reported preferences still hold when choosing between diary approaches in the real world. 

The goal of this project was to design and build a food diary that incorporated what we learned in the previous studies, and investigate whether the findings were applicable in the real world, as opposed to just a lab setting.

\begin{itemize*} 
\item{we wanted to investigate was whether the tradeoff between quality of data collected (that is, time to collect data), and benefit of the data was useful in the real world. 
We hypothesized that there would be a difference between what people chose to do in the lab and what they did in real life-that is, while looking up may be preferred in a lab setting when one has a targeted, possible food and a feeling of ``correctness'', people may choose to use more +1 in the real world, when life intervenes and it's more messy. }
\item{We wanted to see how long people used it. This point is challenging: when we started the study, 3 wks seemed like a long time, but now at the end, it doesn't seem so much. }
\end{itemize*}

There were two separate studies. The first study was an in-lab study similar in design to the BAL-HEI-FBQI study. Participants came to the lab, used the POND tool to make entries specified on a card, then asked for feedback about the tool and experience. The second study was closely related, in that it repeats the first study, but then asks participants to install the POND software on their own personal phone, and to use it to track what they eat for 3 weeks. 

To explore this, we designed and developed an Android-based app with the desired features: namely, a visualization that reflects the pattern of what one has eaten for the day, a quick and easy way to increase the number of items in each group that has been eaten that day, and features to scaffold expectations from more traditional food journals. We tested this in a lab setting, in an experiment similar to the one we ran for the comparison between many food diaries. We found that there was indeed a tradeoff between ``time to enter'' the information: the more components participants were asked to track, the more time it took. The condition with only 2 components was generally less satisfying than the condition with more components, although it was much quicker. 

After the initial lab study, we made a few modifications to the software, then recruited people to use the software to keep track of what they eat for 3 weeks. 

We were interested in exploring how people changed their goals to something they felt reasonable and attainable for the duration of the 3 weeks, so we had 2 conditions: 1 where people could change their goals for the various components, and one where they couldn't (they were asked to keep track of all components). Obvious in retrospect, people just didn't change their goals. They ignored the components that were hard to track or uninteresting, and they focused on the ones they cared about. So that measure was basically useless. Additionally, we needed to encourage people to ``follow the spirit of the law'', so to speak, as some of the calculations for solid fats versus sodium and such were not perfect. Some people naturally did that, while others found that challenging. 

I present the two studies in two sections: The first reflects the in-lab portion of the studies, and the second focuses on the real-world portion of the study. The in-lab portion of the study were the same (same data collected, same tasks used), with minor changes to the software between phases. 


This study is a follow-on to the previous study, and part of a larger study. The larger study consists of two phases: the first phase is an in-lab portion that repeats the study just described, while the second phase requires participants to use the app to track their food intake in their real lives for 3 weeks. Further discussion of phase 2 and the entire study is in the next section. 
For this first phase of this study, participants were asked to do the same in-lab tasks and surveys as described in the previous section. The only modification was some changes to the search functionality of the app. 

The goal of having participants ``duplicate'' the previous study was to ensure that participants would understand the app, have a chance to ask any questions and become familiar with the process of capturing their food. We also wanted to determine if there was a noticeable difference between the 3-wk study population and the in-lab study population in terms of errors, approaches, or whatever. This could also help us evaluate the in-lab study, and understand how performance in the lab study relates to performance in the real world. 

The bigger goal of the tool is to study whether self-directed goal setting, control over the goal you're working on, enables people to keep using the tool longer: both in terms of ability and willingness. We've chosen to design the tool based on a well-defined set of dietary goals, based on an existing standard. One question that comes to mind is whether some goals/components are easier or more difficult to track. We can get this from self-report and from correctness data. 

There are two ways to break up the HEI components: adequacy versus moderation components (this was something mentioned in the previous study), and food groups versus nutrients (this was also mentioned in different ways from participants). 

\section{Evaluation Overview/organization}
This evaluation consisted of both in-lab and in situ  data collection. The study was piloted with an in-lab data collection session. The software was slightly modified based on feedback from the participants, then used for the rest of the study. The second part of the study consisted of participants performing the same tasks as in the pilot in-lab study, followed by them tracking their dietary intake using POND on their personal mobile phone. 

I report and discuss the data in terms of the in-lab portion and the in situ data. When discussing the in-lab data, I treat the pilot and final data in the same way. 

First, I describe the in-lab part of the study. Then, I describe the data collected, report the results and discuss. Next, I describe the in situ part of the study, the data collected in situ, report the results and discuss. Then, I discuss overall, addressing anything that is related between the in-lab and in situ portions. 

\section{In-Lab Evaluation}
The bigger goal of the tool is to study whether self-directed goal setting, control over the goal you're working on, enables people to keep using the tool longer: both in terms of ability and willingness. We've chosen to design the tool based on a well-defined set of dietary goals, based on an existing standard. One question that comes to mind is whether some goals/components are easier or more difficult to track. We can get this from self-report and from correctness data. There are two ways to break up the HEI components: adequacy versus moderation components (this was something mentioned in the previous study), and food groups versus nutrients (this was also mentioned in different ways from participants). 





\subsection{Experiment Design}
\subsubsection{Research Questions}
\begin{enumerate}
\item Investigate the goal-changing stuff
\item Do the TLX measures vary significantly from the in-lab study TLX measures? 
\item Sanity check

\end{enumerate}

\subsubsection{Study Design Limitations}
Similar to those in the previous in-lab study described in section x.y.z. Primarily, that we are using foods that people may not be familiar with. Additionally, the scripted nature of the tasks means that participants didn't get a chance explore features of the software and become more familiar with it. Also, this is a tool that is intended to be quicker and easier over an extended period of use (that is, it's likely to be more challenging at the beginning of use, because it forces people to think about the food their entering a little bit differently, but easier as one learns and adapts). Also, the situation and context matter-many food diaries are evaluated positively in a lab setting, but fail in a real-world situation. The in-lab study clearly has limitations as to how we can use feedback about how valuable this tool can be, but data about the types of errors people make (initially) and the relative time comparisons can be useful. 


\subsubsection{Procedure}
1)	Collect preliminary/demographic data
2)	Do the training
a)	Go over the ``cheat sheet'', explaining each component. 
b)	Give some examples
c)	Explain the procedure
i)	Participants were shown a card with a meal on it; they started the app by touching the icon; they entered the food as they desired (either +1 buttons or looking up); when they were done entering all the foods on the card, they were asked to use the back button to end the task. 

If the participant made a mistake, they were asked to note it to the administrator, but not to fix it. The app was designed such that entering was very easy, but fixing a mistake took longer. For this study, we wanted to be able to capture the entry time not confounded by edit time. We did want to capture the distinction between an error the participant knew they made, and errors they were unaware of. 

3)	Condition S-M-L: 
a)	Small (2), Medium (5) and Large (9) groups of components
b)	Chosen randomly without replacement; Some are going to have to repeat after all others have been used (again, randomly chosen)
c)	TLX survey
4)	4 groups of 5 meals each (B-L-D-SS-BS)
a)	Always presented in the same order
b)	Condition order is counterbalanced
5)	Final survey: questions about which items they liked/didn't like, etc. 

FIGURE HERE: Screenshots of example conditions (2 goals, 5 goals, etc). 

\subsubsection{TASKS}
Same as developed and used in previous in-lab study. 
One concern: The tasks were developed using an earlier version of the food database than was used for the POND database. The database was by the same manufacturer, but it was a newer version. One of the modifications was that some food names were different. Specifically, the new food names had a different ordering strategy than the previous one. So, while all the same foods were in the database, the food names were a little bit different. 


\subsubsection{Participants-- Pilot}

12 participants recruited from email lists for a major university. All participants were students or instructors in a technology-centric field (Computer Science \& Engineering, Informatics,  and Human-Centered Design \& Engineering). 6 male, 6 female.  All reported using a cell phone several times a day, and all but one reported entering text on their cell phone several times per day (the other one reported entering text on their cell phone 1-2 times per day). All participants owned smartphones, 2 participants owned smartphones that didn't have a touch-screen and 4 owned smartphones with QWERTY keyboards. 4 reported using a food diary previously. Of the 4, all had used a food diary on a smartphone, while one of those 4 had also used paper and either software or a website. When asked about health concerns and goals, 11 people indicated that they are interested in ``eating better'', 6 are interested in losing weight, 2 want to better control portion sizes, and 2 are interested in eating less or more. Other answers volunteered included ``Eating fewer processed foods'', and ``Eating only healthy/no junk food''. When asked to rate on a scale of 1-4 (very knowledgeable) how knowledgeable they are about food and nutrition (``including nutrients (fat, carbohydrates, fiber, vitamins, etc) and/or ingredients''), 5 reported themselves as a 2, 6 reported themselves as a 3, and 1 reported themselves as 4, or that they ``are expert/have spent much time understanding nutritional aspects of food.''

Participants were compensated for their time with a \$10 Amazon gift certificate.
\subsubsection{Participants}
22 people participated in this study, 17 female and 5 male. Ages ranged from 21-64, and occupations varied. All participants reported using their cell phone several times a day, and all except 1 reported entering text on their cell phones several times a day (the remaining one entered text on their cell phones 1-2 times daily). All participants owned cell phones with touch screens, and 2 owned phones with an additional QWERTY keyboard. 19 participants reported that one of their health goals is to ``Eat better'', while 8 reported wanting to lose weight, 8 wanted to better control portion sizes, and 4 wanted to eat less or more. 6 people reported never tracking their food intake before, while 7 had used a smart phone, 8 had used a website, 8 had used paper, and 1 used some other software on their computer. 7 reported themselves as very knowledgeable about food and nutrition, 10 as fairly knowledgeable, 4 as not so knowledgeable, and 1 as fairly uneducated about food and nutrition.  

Participants were recruited from the community via craigslist posting, a recruitment request sent to a local mom's email list, posters in the neighborhoods at cafes, bookstores, and athletic shops, as well as some email lists at the university that were different than the ones used in previous studies. Participants were compensated \$125 for their complete participation in the study. Participants self-reported that they had no medical concerns that impacted their food choices, and owned their own Android devices. 

Some participants did the in-lab part, but were unable to complete the second in-situ part for whatever reasons. 

\subsection{Data Collected/Measures}
\begin{enumerate*}
\item Entry Strategy (how)
\item Correctness/Errors
\item Timing/duration
\item TLX
\item Length of use
\item Likes/dislikes
\item Search terms
\end{enumerate*}

\subsubsection{Entry Strategy (How)}

For each task, a participant could have accomplished the task by either looking the foods up in the database or using the +1 buttons. I classified each task as ``+1'' entry, ``lookup'' entry, or ``mixed'' entry. 


\subsubsection{TLX}
A TLX survey was administered at the end of each condition. 

\subsubsection{LENGTH OF USE}

At the end of the study, participants were asked: 
\begin{itemize*}
\item Which components could you track for 3 days? 
\item Which components could you track for 3 weeks? 
\item Which components could you track for 3 months? 
\item How likely are you to succeed in tracking your selected items for 3 days/weeks/months? 
\item How likely are you to succeed in tracking all components for 3 days/weeks/months? 
\end{itemize*}

\subsubsection{Likes/Dislikes}

After completing each condition, participants were asked to list 3 things they liked or disliked about that condition. 

One concern about this was that some participants didn't enter anything for the entries, because their feedback was the same for each condition. The only change was the number and which of the categories appeared on the front screen. 

Need to compile!

\subsubsection{SEARCH TERMS}

For the participants who used the food lookup feature, what queries they used to find a given food. 

\subsection{Hypotheses}

\subsection{Analysis and Results}
\subsubsection{ANALYSIS APPROACH}

\subsection{Results}

\subsubsection{Entry Strategy}
 \textbf{Pilot data:}
3 participants chose to use the +1 buttons exclusively, while one other participant used the +1 buttons primarily, but tried the lookup function a bit. Of the remaining 8 participants, 4 mostly used the lookup function, 1 used lookup or a combination exclusively, and the last 3 used a combination of the three strategies. 

\textbf{data:}
7 participants chose to use the +1 buttons exclusively, while three other participants used the +1 buttons primarily, but tried the lookup function a bit. Of the remaining 14 participants, 4 mostly used the lookup function,  none used lookup or a combination exclusively, and the last 10 used a combination of the three strategies. 

\[Could show the differences in counts between this phase and the previous\]

1.5.2	CORRECTNES


S

1.5.3	TIMING/DURATION
I feel like not doing anything with this, because I can't really say anything interesting about it. 
1.5.4	TLX

\subsubsection{Length of use}

 \textbf{Pilot data chart goes here}

** Get rid/check the Sat Fat item!!
Pretty consistently, most participants felt they could probably track the food group components for three days, while as the time gets longer they reported less confidence in their ability to keep track. For the most part, participants did not feel confident about tracking sugar, fats, oils, and sodium for any length of time. 

\textbf{data chart goes here} 
Here, there seems to be a similar trend as before, but it does appear that this group is more confident about tracking the fats/sugar/sodium. 

\subsubsection{Search Results}
I believe there were no search result entries for the pilot data. 

I need to look at a table of: task ID/task terms, all search queries made for each task. Overall, a breakdown of all terms that were used, how many people used the same term. 

There were 710 queries from 277 unique terms made by the \#n participants. 134 query terms were used by more than one person. 

This seems high. In the in-situ part, participants made 400-ish queries, and that was 3 weeks, not ~4 days of food. 




\subsubsection{LIKES/DISLIKES}

\textbf{SMALL CONDITION}
Some people liked that it was so short (``it was trying to just keep track of my best and worst food choices''), while others felt it was too limiting ``I wanted to put all of the food I ate into categories, not just some of it''. 

\textbf{MEDIUM CONDITION}
Fewer items, so it takes less time, but more ``mentally taxing'', because they ``had to think more about whether a food contained parts of the specified categories''. One person mentioned s/he liked ``Quicker entry for foods I've encountered before''. 

\textbf{FULL CONDITION}
Liked having all the categories. Liked that it made them more informed, covering all the parameters. 

\textbf{OVERALL/APPLIES TO ALL CONDITIONS}
Database-having so many items, but not ``all items sold in stores''. No easy undo (limitation of the study protocol). Liked the ability to search overall. Learning curve to figuring out what food falls into what category. Liked color coded categories of food, and felt they could learn the components and portion sizes over time. Did not like the lack of feedback or uncertainty of whether something was added correctly. Fun to hit the +1 buttons. Liked the colors, the ``meter''. Search results were too wordy and took too long to find the exact food and portion size. Made me think about the nutritional value of what I'm eating, and it keeps me informed of my habits. Even though it takes too many steps to enter a food item, I'd prefer to specify the food I eat and just get the analysis back. 

\textbf{notes from non-pilot}
I don't know if this goes here or in discussion, but I definitely think that people gave more thoughtful responses in this version of the in-lab study, as opposed to the previous one where people were just doing the in-lab part. 

\subsection{Search Terms}

[didn't collect search terms for this part-just the foods actually entered]

\subsection{Discussion}

Points I'm observing in the data: 
\begin{itemize*}
\item There was a split in responses. Some liked that it was quick to enter simple foods with the +1 buttons, and many people liked the overall concept-that it gave an overview of the quality of the diet, showing where certain components could improve. 

\item In general, people felt uncomfortable when components were missing. It seemed difficult to adapt to, and people didn't like that sometimes they couldn't account for every single food they ate. 

\item People really wanted the search feature, but definitely wanted it improved. 

\item Some people liked that they could be less accurate about entering something prepared for them (for example, restaurant food), while others wanted more detail. 

\end{itemize*}



\section{In-situ Study}
The bigger goal of the tool is to study whether self-directed goal setting, control over the goal you're working on, enables people to keep using the tool longer: both in terms of ability and willingness. We've chosen to design the tool based on a well-defined set of dietary goals, based on an existing standard. One question that comes to mind is whether some goals/components are easier or more difficult to track. We can get this from self-report and from correctness data. There are two ways to break up the HEI components: adequacy versus moderation components (this was something mentioned in the previous study), and food groups versus nutrients (this was also mentioned in different ways from participants). 

\subsection{Experiment Design}
\subsubsection{Research Questions}

\begin{itemize*}
\item Do the findings/observations in the lab match the experience in the real world? 
\item How do they differ? 
\item Original question: Goal-changes and stuff
\end{itemize*}

\subsubsection{Study Design Limitations}
As with all in situ food diary evaluations, we can't capture what is "truth": what people really eat, and how they enter it.
 
\subsubsection{Participants}
Same as described above in Part 2. 

\subsubsection{Procedure}
Participants performed the in-lab study as described in Part 1/2 above. When they were completed with the in-lab part of the study, the software was installed on the participant's personal Android phones. Participants were shown software features not addressed in the lab part of the study, such as how to edit entries, create custom shortcuts/meals, how to view a list of daily entries over time, and the weekly overview. They were given a chance to interact with the software, and ask any questions. They were told to play with the software for the rest of the day, but that the study started the next morning. They were told to customize the components and goals as they desired, and that the serving sizes were there as a guideline, but that they could choose to count the servings as they felt comfortable. 

At the end of every 7 days, participants were asked to send a copy of their data to the research team, and asked to fill out a survey about their experience for the previous week. 

At the end of the 3 week period, participants returned to the lab for a guided interview about their experiences. At that point, the software was removed from their personal device. 

\subsection{Data Collected/Measures}

Full study interim surveys:  (1 wk, 2 wk, 3 wk)
\begin{itemize*}
\item 3 things you like/don't like
\item Where is it effective/not effective
\item Difficulty/interesting questions (rate each component)
\end{itemize*}

At the end of the study: 

Guided interview

At this point, I'm getting bogged down: Some measures were collected for this part of the study (for the most part, the end-of-week surveys), and I haven't done anything with that data yet. The question is I don't know what data to deal with apart from the interview qualitative data yet. So, I need to leave this be for now, apart from that interview data, and continue. 

For purposes of discussion, I would like to define two classes of data I am interested in-indicators of engagement, and indicators of success. Indicators of engagement reflect how much the participant actually used the tool they are given. This includes things like how much time was spent using the tool, how many entries were made, and how many days the tool was used. Indicators of success reflect the progress of the participant toward the given goal of ``improving dietary intake''. This includes things like the number of servings of targeted food groups per day, changes in self-efficacy, and changes in the participant's stage of change. 

\subsubsection{RANDOM BRAINSTORMING THINGS TO LOOK AT}
\begin{itemize*}
\item Self-efficacy questions at the end of each week
\item Self-report of "+1 versus food lookup", compared to actual performance
\item Compare self-report at end of lab portion to what they actually did
\item Pattern of use over three weeks
\item Correlation between weekly self-report of use and actual use
\end{itemize*}

\subsubsection{Entry Strategy}
For this phase of the study, we couldn't calculate the same strategy measures as in the lab studies, since we don't know what ground truth is (we can't calculate strategy per task). Instead, I report on how many +1 versus lookup entries participants made. 

Noting this here, though it may need to go elsewhere: there were two participants who preferred using the lookup feature. After 8-10 days of use, the app stopped behaving properly. They eventually reverted to using the +1 buttons rather than the preferred lookup, and reported that they would have preferred to keep looking up. However, their databases were corrupted, so I don't have their data anyway. 

\subsubsection{Overall Component Counts}

\subsubsection{Timing/Duration}

How much time they spent using the app over the course of three weeks. Average amount of use per episode of use. I don't have this because I didn't log when a user stopped using the app (that is, when the back button was pressed and the app was closed). This is partly due to the Android platform. On the Android platform, one could either use the back button to close the app, or use another tactic to switch the app that one is using. Android manages when apps get closed/shutdown entirely, so there is some uncertainty around when someone actually stops using it. It's also a very strong Android convention to not override the back button, even simply to log the pressing of the back button, so I was loath to do it unless it would give us valuable information. In retrospect, I should've logged it. 

\subsubsection{Use over time}
Patterns of use. How many times per day, entries per day, entries over time were made. 

I calculated the number of entries that were made in a particular time period on each day. TODO: look up in the code exactly how it counted entries per time period. Days were broken into 8-3hr periods. Entries made in the given day and period were counted (eg, Day 1, Period 1 is 0 entries, but Day 1 Period 2 (3-6am) has 3 entries). This allows us to compare the pattern of use from day to day and from person to person. 

In the BALANCE study, I started to look at the number of times the app was started in a time period. This was a rough measure and definitely not representative of actual use. In this study, because participants used their own phone, it more closely reflected how the app was likely used over time. However, sometimes the app is started just to review or "play" with, and sometimes the app is started multiple times for one period of entry. It also doesn't capture "how much work" was done with the app in a given period of time, whereas entries captures the amount of activity a bit better. I still didn't calculate it yet though, and might not. 

\subsubsection{LIKES/DISLIKES}
Participants were asked their likes and dislikes about the app over time: After each condition of the in-lab study (discussed earlier), at the end of the in-lab study (discussed earlier), at the end of each week in a web-based survey, and at the end of the study in a guided interview (discussed further below). These were free-form, volunteer answers. 

\subsubsection{Guided Interviews}
At the end of the study, participants were asked to return to the lab to conduct an in-person interview about their use of the tool. The interview protocol is attached as an appendix, as is the coding documentation. 

\subsection{Hypotheses}

\subsection{Data Analysis}
\subsubsection{ANALYSIS APPROACH}

\subsubsection{ENTRY STRATEGY}

\subsubsection{OVERALL COMPONENT COUNTS}

\subsubsection{USE OVER TIME }
\subsubsection{SEARCH TERMS }
\subsubsection{GUIDED INTERVIEW}


\subsection{Results}
Something goes here. 

(table Taken from PondSummary.xlsx, Sheet4, PivotTable for 'Summary' sheet)

When I look at this, I wonder if the numbers reflect what they did in the lab…

\subsubsection{OVERALL COMPONENT COUNTS}

\subsubsection{USE OVER TIME}

(Taken from PondSummary.xlsx, Sheet4, the pivot table for 'Summary' sheet)
Above is a table (temporarily) that shows how many entries were made in the specified time period, summed over the duration of the study. We see that a couple of participants (p1003, p3021) had two periods in the day when they definitely made more entries, and a couple of other participants show similar trends, but not so distinctly. A few participants show pretty consistent use throughout the day (p3024, p3011). 

Questions: did p1003 or p3021 (or even p2002, p3002) make comments about their routine or schedule that can explain 

\subsubsection{SEARCH TERMS}

There were 465 food queries from 308 unique terms, made by 18 participants. 76 query terms were used more than one time, with 42 query terms used by more than one person. Details about the queries are included in the appendix. 

\subsubsection{GUIDED INTERVIEW}

In this section, identify the bigger themes/questions I asked or probed on, and then the groups of answers that were generated. 

Themes Identified: 

When people used the tool: A few people reported using it consistently after eating, while others made comments that some meals were easy to remember to enter because they are at times that their routine is consistent or predictable; other meals were at times when the routine is unpredictable. Participants also commented about how their schedules changed week to week: participants who had been traveling noted that their schedules were just off overall. Some people changed schedules over the study: one participant noted that there was one week of classes that was "predictably crazy", one week of vacation that was relaxed and easy to pay attention, and one week of starting an internship rotation in a venue that was high-pressure and busy, where it was hard to remember or want to think about entering. Other issues that prevented people from making entries immediately after eating included that their phone wasn't with them (frequently commented that it was elsewhere, charging) or they were doing something else while eating and forgot. 

Many comments were about what features people would like. An improved food database and searching process were frequently mentioned. People also wanted to track their water intake, and some people wanted some notion of calorie intake. In regards to calories, some people wanted detailed caloric information, while others reported being comfortable with an estimate. People also talked about wanting more customization: the order of the items on the screen, the background color, which items appear on the front screen. A couple of people volunteered the notion of a "Junk food" button-to indicate that they had something with low nutritional value, but with less detail than specifying how much sugar, salt, or fat it contains. 

One of the things people really liked was the distinction between whole grains and refined grains, and dark green/orange veggies and all other veggies. They also liked the overview/analysis this approach provided, and that the colors made it easy to think about. Many people had trouble tracking sugar, sodium and fats (predictably), and some people chose to ignore them (partly due to the challenge, and partly due to caring about it). 

When asked about things people didn't want to record, one response was "things I wasn't sure how to spell". 

People talked about how they made a decision between using the +1 feature and looking food up in the database. Some people preferred looking up, but ended up using the +1 feature for various difficulties associated with the lookup process: they were uncertain what phrases to use to find the food, they never found the food they wanted, they didn't trust the results of the database (eg, Subway sandwich with no grains), and the software was crashing on some lookups. [how many people reported these]. Others chose to use the +1 as much as possible, and some took the approach of looking up anything prepared/"had a barcode"/from a restaurant, while using the +1 for anything prepared "at home". 
Something I didn't initially ask specifically about, but did eventually: Some people were satisfied with their current eating patterns/behaviors, some knew they needed to change, some just didn't seem too concerned. 

Some people were unclear if the goals were to be attained or moderated. 

Some people talked about how the tool helped them to think more about the choices they were making, and they ended up eating more whole grains, choosing more vegetables, eating or buying more fruits, or choosing not to eat something that would increase the yellows and orange categories. Some people were surprised about some of the nutrients they encountered: things like sugar in slushy, sodium in soup, or that they really didn't consume as many whole grains as they believed they did. 

When asked who they would recommend this tool to, many people volunteered that it would be really good for senior citizens, because it's so easy to use and keeps things so simple. A couple others indicated that they thought it would be good for busy mothers and caregivers, not just to track for themselves but to also keep track of what children (or family members) eat, and provide communication around that. People also thought it might be good for people who were just starting to try to figure out how to make changes in their diet. A couple people thought it wouldn't be useful for anyone, because it was just too vague, while others thought it could be useful to anyone, because it is simple and gives high level feedback and awareness about one's patterns. 

We asked people to volunteer coarse location information for entries. No one did so. In general, the feedback on this feature was that they didn't think it mattered, they didn't care about analyzing where they ate food, and it was just another thing to do, so they chose not to do it. A couple of people volunteered that they might be able to imagine there are patterns in the data, but they didn't think there was likely any insight or value from it. 

I also asked people why they participated in the study. Some people did it purely for personal interest, stating that generally they are interested in tools to support their own health and wellness, while others did it just for the money. A couple of people had a balance of the two. 

Data detail tradeoff: 

Increased awareness of food composition: 

Sharing with doctor/trainer/coach: 

Sharing with peers/support group: 

\subsection{Discussion}

\section{Observations and Discussion}
For organizational purposes at this point, breaking it down. In this section I'm going to talk about the entries people made. 

\subsection{How}

How did individuals make an entry? Did they use the +1 or the lookup? This is interesting in all 3 phases. 

\subsection{When}
When did participants make entries? What were some of the patterns of use that people had? Consistent through the day, or all bunched together at night? 

\subsection{What}

What entries did they make? Would this be a discussion about what kinds of foods people tracked, maybe a discussion about what groups had high counts either from +1 or lookup, or maybe a discussion about what kinds of things they looked up and subsequently entered? 

\subsection{Where}
We don't have much data to explain this, but people did make comments about whether work was consistent or not, versus consistency at home. 

\subsection{Why}
What were some people's motivations for choosing either the +1 approach or the lookup approach? Or motivation for participating at all? 


What we did see: 
\begin{itemize*}
\item POND helped people keep track of less-processed food. More-processed food was more difficult and depended on the database. 
\item Schedules/routines matter. 
\item I wonder: personal profiles. 
\item People liked the big view/analysis
\item Ownership/personalization: They wanted to own it and personalize, but they didn't try very hard. 
\item They didn't go deep into the experience. 
\end{itemize*}


\section{Future Work}

\section{Thoughts on the methodology}

"	I would personalize questions, either on surveys throughout the study, or at least at the end. To capture things like "do you still agree with this thought you reported having at the beginning of the study?"
"	


